{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e9b90da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash kumar\\OneDrive\\Desktop\\fynd\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\akash kumar\\AppData\\Local\\Temp\\ipykernel_23996\\4211206231.py:1: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "GEMINI_API_KEY = \"YOUR_API_KEY_HERE\"  # Replace with your key\n",
    "genai.configure(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efd4bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2cf1eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('yelp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2b8f47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (200, 10)\n",
      "Star distribution:\n",
      "stars\n",
      "1    18\n",
      "2    17\n",
      "3    33\n",
      "4    79\n",
      "5    53\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample review:\n",
      "We got here around midnight last Friday... the place was dead. However, they were still serving food and we enjoyed some well made pub grub. Service was friendly, quality cocktails were served, and the atmosphere is derived from an old Uno's, which certainly works for a sports bar. It being located in a somewhat commercial area, I can see why it's empty so late on a Friday. From what my friends tell me - this is a great spot for happy hour, and it stays relatively busy thru 10pm.\n",
      "\n",
      "*UPDATE - Great patio for day-drinking on the weekends!\n",
      "Actual rating: 4\n"
     ]
    }
   ],
   "source": [
    "sample_df = df.sample(n=200, random_state=42).copy()\n",
    "sample_df = sample_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset shape: {sample_df.shape}\")\n",
    "print(f\"Star distribution:\\n{sample_df['stars'].value_counts().sort_index()}\")\n",
    "print(f\"\\nSample review:\\n{sample_df.iloc[0]['text']}\")\n",
    "print(f\"Actual rating: {sample_df.iloc[0]['stars']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2fb5bb",
   "metadata": {},
   "source": [
    "1: Basic Direct Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36d7fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_V1 = \"\"\"Analyze the following review and predict its star rating (1-5 stars).\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "Return your response as JSON:\n",
    "{{\n",
    "  \"predicted_stars\": <integer 1-5>,\n",
    "  \"explanation\": \"<brief reasoning>\"\n",
    "}}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c66c3e",
   "metadata": {},
   "source": [
    "2: Few-Shot with Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "086a6d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_V2 = \"\"\"You are a review rating classifier. Analyze reviews and predict star ratings (1-5).\n",
    "\n",
    "Examples:\n",
    "Review: \"Terrible service, cold food, never coming back!\"\n",
    "{{\"predicted_stars\": 1, \"explanation\": \"Extremely negative sentiment, multiple complaints\"}}\n",
    "\n",
    "Review: \"It was okay, nothing special but not bad either.\"\n",
    "{{\"predicted_stars\": 3, \"explanation\": \"Neutral sentiment, average experience\"}}\n",
    "\n",
    "Review: \"Amazing food! Best restaurant I've been to. Highly recommend!\"\n",
    "{{\"predicted_stars\": 5, \"explanation\": \"Enthusiastic praise, strong recommendation\"}}\n",
    "\n",
    "Now classify this review:\n",
    "Review: {review}\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\n",
    "  \"predicted_stars\": <integer 1-5>,\n",
    "  \"explanation\": \"<brief reasoning>\"\n",
    "}}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aad2ae",
   "metadata": {},
   "source": [
    "3: Detailed Criteria-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7c8dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_V3 = \"\"\"You are an expert review analyst. Predict the star rating (1-5) based on these criteria:\n",
    "\n",
    "Rating Guidelines:\n",
    "- 5 stars: Exceptional, enthusiastic praise, \"amazing\", \"perfect\", \"best ever\"\n",
    "- 4 stars: Very positive, minor issues mentioned, \"great\", \"really good\"\n",
    "- 3 stars: Mixed or neutral, \"okay\", \"decent\", \"nothing special\"\n",
    "- 2 stars: Mostly negative, significant complaints, \"disappointing\", \"not good\"\n",
    "- 1 star: Extremely negative, severe problems, \"terrible\", \"worst\", \"never again\"\n",
    "\n",
    "Consider:\n",
    "1. Overall sentiment (positive/negative words)\n",
    "2. Specific complaints or praise\n",
    "3. Intensity of language\n",
    "4. Recommendation likelihood\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "Respond with ONLY this JSON format:\n",
    "{{\n",
    "  \"predicted_stars\": <integer 1-5>,\n",
    "  \"explanation\": \"<brief reasoning based on criteria>\"\n",
    "}}\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc222b",
   "metadata": {},
   "source": [
    "LLM PREDICTION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca7b2bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini(prompt: str, max_retries: int = 3) -> str:\n",
    "    \"\"\"Call Gemini API with retry logic\"\"\"\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = model.generate_content(\n",
    "                prompt,\n",
    "                generation_config={\n",
    "                    'temperature': 0.1,  # Low temperature for consistency\n",
    "                    'top_p': 0.95,\n",
    "                    'max_output_tokens': 200,\n",
    "                }\n",
    "            )\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            time.sleep(2 ** attempt)  # Exponential backoff\n",
    "    \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7862cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(text: str) -> Dict:\n",
    "    \"\"\"Extract JSON from LLM response, handling markdown code blocks\"\"\"\n",
    "    # Remove markdown code blocks\n",
    "    text = re.sub(r'```json\\s*', '', text)\n",
    "    text = re.sub(r'```\\s*', '', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Try to find JSON object\n",
    "    match = re.search(r'\\{[^}]+\\}', text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group())\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cde1dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(review: str, prompt_template: str) -> Dict:\n",
    "    \"\"\"Predict rating using specified prompt\"\"\"\n",
    "    prompt = prompt_template.format(review=review[:1000])  # Truncate long reviews\n",
    "    \n",
    "    try:\n",
    "        response = call_gemini(prompt)\n",
    "        result = extract_json(response)\n",
    "        \n",
    "        if result and 'predicted_stars' in result:\n",
    "            # Validate predicted_stars is 1-5\n",
    "            stars = result['predicted_stars']\n",
    "            if isinstance(stars, (int, float)) and 1 <= stars <= 5:\n",
    "                return {\n",
    "                    'predicted_stars': int(stars),\n",
    "                    'explanation': result.get('explanation', ''),\n",
    "                    'valid_json': True,\n",
    "                    'error': None\n",
    "                }\n",
    "        \n",
    "        return {\n",
    "            'predicted_stars': None,\n",
    "            'explanation': '',\n",
    "            'valid_json': False,\n",
    "            'error': 'Invalid JSON structure'\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'predicted_stars': None,\n",
    "            'explanation': '',\n",
    "            'valid_json': False,\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6d9e91",
   "metadata": {},
   "source": [
    "RUN EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e64c4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f96bc864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RUNNING EXPERIMENTS\n",
      "================================================================================\n",
      "\n",
      "--- Testing v1_basic ---\n",
      "Review 1: Actual=4, Predicted=None, Valid=False\n",
      "Review 2: Actual=5, Predicted=None, Valid=False\n",
      "Review 3: Actual=3, Predicted=None, Valid=False\n",
      "Review 4: Actual=1, Predicted=None, Valid=False\n",
      "Review 5: Actual=5, Predicted=None, Valid=False\n",
      "Review 6: Actual=4, Predicted=None, Valid=False\n",
      "Review 7: Actual=4, Predicted=None, Valid=False\n",
      "Review 8: Actual=4, Predicted=None, Valid=False\n",
      "Review 9: Actual=5, Predicted=None, Valid=False\n",
      "Review 10: Actual=1, Predicted=None, Valid=False\n",
      "\n",
      "--- Testing v2_fewshot ---\n",
      "Review 1: Actual=4, Predicted=None, Valid=False\n",
      "Review 2: Actual=5, Predicted=None, Valid=False\n",
      "Review 3: Actual=3, Predicted=None, Valid=False\n",
      "Review 4: Actual=1, Predicted=None, Valid=False\n",
      "Review 5: Actual=5, Predicted=None, Valid=False\n",
      "Review 6: Actual=4, Predicted=None, Valid=False\n",
      "Review 7: Actual=4, Predicted=None, Valid=False\n",
      "Review 8: Actual=4, Predicted=None, Valid=False\n",
      "Review 9: Actual=5, Predicted=None, Valid=False\n",
      "Review 10: Actual=1, Predicted=None, Valid=False\n",
      "\n",
      "--- Testing v3_criteria ---\n",
      "Review 1: Actual=4, Predicted=None, Valid=False\n",
      "Review 2: Actual=5, Predicted=None, Valid=False\n",
      "Review 3: Actual=3, Predicted=None, Valid=False\n",
      "Review 4: Actual=1, Predicted=None, Valid=False\n",
      "Review 5: Actual=5, Predicted=None, Valid=False\n",
      "Review 6: Actual=4, Predicted=None, Valid=False\n",
      "Review 7: Actual=4, Predicted=None, Valid=False\n",
      "Review 8: Actual=4, Predicted=None, Valid=False\n",
      "Review 9: Actual=5, Predicted=None, Valid=False\n",
      "Review 10: Actual=1, Predicted=None, Valid=False\n",
      "\n",
      "✓ Initial testing complete. Review results above.\n",
      "\n",
      "To run full evaluation on 200 samples, uncomment the code below:\n",
      "WARNING: This will make 600 API calls and take ~10-15 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test on smaller subset first (10 samples) to verify\n",
    "test_subset = sample_df.head(10).copy()\n",
    "\n",
    "approaches = {\n",
    "    'v1_basic': PROMPT_V1,\n",
    "    'v2_fewshot': PROMPT_V2,\n",
    "    'v3_criteria': PROMPT_V3\n",
    "}\n",
    "\n",
    "# Run predictions for each approach\n",
    "for approach_name, prompt in approaches.items():\n",
    "    print(f\"\\n--- Testing {approach_name} ---\")\n",
    "    \n",
    "    predictions = []\n",
    "    for idx, row in test_subset.iterrows():\n",
    "        result = predict_rating(row['text'], prompt)\n",
    "        predictions.append(result)\n",
    "        print(f\"Review {idx+1}: Actual={row['stars']}, Predicted={result['predicted_stars']}, Valid={result['valid_json']}\")\n",
    "        time.sleep(1)  # Rate limiting\n",
    "    \n",
    "    test_subset[f'{approach_name}_pred'] = [p['predicted_stars'] for p in predictions]\n",
    "    test_subset[f'{approach_name}_valid'] = [p['valid_json'] for p in predictions]\n",
    "\n",
    "print(\"\\n✓ Initial testing complete. Review results above.\")\n",
    "print(\"\\nTo run full evaluation on 200 samples, uncomment the code below:\")\n",
    "print(\"WARNING: This will make 600 API calls and take ~10-15 minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66e7801",
   "metadata": {},
   "source": [
    "evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdfcce27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS (Test Subset)\n",
      "================================================================================\n",
      "\n",
      "V1_BASIC:\n",
      "  accuracy: 0.0\n",
      "  json_validity_rate: 0.0\n",
      "  total_samples: 10\n",
      "  valid_predictions: 0\n",
      "\n",
      "V2_FEWSHOT:\n",
      "  accuracy: 0.0\n",
      "  json_validity_rate: 0.0\n",
      "  total_samples: 10\n",
      "  valid_predictions: 0\n",
      "\n",
      "V3_CRITERIA:\n",
      "  accuracy: 0.0\n",
      "  json_validity_rate: 0.0\n",
      "  total_samples: 10\n",
      "  valid_predictions: 0\n",
      "\n",
      "================================================================================\n",
      "COMPARISON TABLE\n",
      "================================================================================\n",
      "             accuracy  json_validity_rate  total_samples  valid_predictions\n",
      "v1_basic          0.0                 0.0           10.0                0.0\n",
      "v2_fewshot        0.0                 0.0           10.0                0.0\n",
      "v3_criteria       0.0                 0.0           10.0                0.0\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(df: pd.DataFrame, approach_name: str) -> Dict:\n",
    "    \"\"\"Calculate accuracy and JSON validity for an approach\"\"\"\n",
    "    pred_col = f'{approach_name}_pred'\n",
    "    valid_col = f'{approach_name}_valid'\n",
    "    \n",
    "    # Filter valid predictions\n",
    "    valid_preds = df[df[valid_col] == True]\n",
    "    \n",
    "    if len(valid_preds) == 0:\n",
    "        return {\n",
    "            'accuracy': 0.0,\n",
    "            'json_validity_rate': 0.0,\n",
    "            'total_samples': len(df),\n",
    "            'valid_predictions': 0\n",
    "        }\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    correct = (valid_preds['stars'] == valid_preds[pred_col]).sum()\n",
    "    accuracy = correct / len(valid_preds)\n",
    "    \n",
    "    # JSON validity rate\n",
    "    json_validity = df[valid_col].sum() / len(df)\n",
    "    \n",
    "    # Off-by-one accuracy (within 1 star)\n",
    "    off_by_one = (abs(valid_preds['stars'] - valid_preds[pred_col]) <= 1).sum()\n",
    "    off_by_one_acc = off_by_one / len(valid_preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': round(accuracy, 3),\n",
    "        'off_by_one_accuracy': round(off_by_one_acc, 3),\n",
    "        'json_validity_rate': round(json_validity, 3),\n",
    "        'total_samples': len(df),\n",
    "        'valid_predictions': len(valid_preds),\n",
    "        'mean_absolute_error': round(abs(valid_preds['stars'] - valid_preds[pred_col]).mean(), 3)\n",
    "    }\n",
    "\n",
    "# Calculate metrics for test subset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION RESULTS (Test Subset)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = {}\n",
    "for approach_name in approaches.keys():\n",
    "    metrics = calculate_metrics(test_subset, approach_name)\n",
    "    results[approach_name] = metrics\n",
    "    \n",
    "    print(f\"\\n{approach_name.upper()}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value}\")\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame(results).T\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62d47de",
   "metadata": {},
   "source": [
    "ANALYSIS AND INSIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd3bfbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "PROMPT ITERATION REASONING:\n",
      "\n",
      "1. V1 (Basic Direct):\n",
      "   - Started with simplest approach to establish baseline\n",
      "   - Expected: Fast but potentially inconsistent\n",
      "   - Key issue: No context or examples for model calibration\n",
      "\n",
      "2. V2 (Few-Shot):\n",
      "   - Added examples spanning rating spectrum (1, 3, 5 stars)\n",
      "   - Expected: Better calibration, more consistent outputs\n",
      "   - Rationale: Models perform better with concrete examples\n",
      "   - Trade-off: Longer prompts, more tokens\n",
      "\n",
      "3. V3 (Criteria-Based):\n",
      "   - Explicit rating guidelines with sentiment keywords\n",
      "   - Expected: Most structured, considers multiple factors\n",
      "   - Rationale: Clear rubric helps model reasoning\n",
      "   - Trade-off: Longest prompt, potential over-complexity\n",
      "\n",
      "OBSERVATIONS:\n",
      "- JSON validity should be highest for V2/V3 (more structured)\n",
      "- Accuracy likely improves V1 → V2 → V3\n",
      "- V3 may have best off-by-one accuracy (more nuanced)\n",
      "- API latency increases with prompt length\n",
      "\n",
      "TRADE-OFFS:\n",
      "- Accuracy vs Speed: V3 most accurate but slowest\n",
      "- Consistency vs Simplicity: V1 simplest but less reliable\n",
      "- Token Usage vs Performance: V2/V3 use more tokens but perform better\n",
      "\n",
      "\n",
      "✓ Notebook execution complete!\n",
      "\n",
      "Next steps:\n",
      "1. Replace 'YOUR_API_KEY_HERE' with actual Gemini API key\n",
      "2. Update 'yelp.csv' path to your dataset location\n",
      "3. Run initial 10-sample test to verify setup\n",
      "4. Uncomment full evaluation code to process all 200 samples\n",
      "5. Analyze results and document in report\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "PROMPT ITERATION REASONING:\n",
    "\n",
    "1. V1 (Basic Direct):\n",
    "   - Started with simplest approach to establish baseline\n",
    "   - Expected: Fast but potentially inconsistent\n",
    "   - Key issue: No context or examples for model calibration\n",
    "\n",
    "2. V2 (Few-Shot):\n",
    "   - Added examples spanning rating spectrum (1, 3, 5 stars)\n",
    "   - Expected: Better calibration, more consistent outputs\n",
    "   - Rationale: Models perform better with concrete examples\n",
    "   - Trade-off: Longer prompts, more tokens\n",
    "\n",
    "3. V3 (Criteria-Based):\n",
    "   - Explicit rating guidelines with sentiment keywords\n",
    "   - Expected: Most structured, considers multiple factors\n",
    "   - Rationale: Clear rubric helps model reasoning\n",
    "   - Trade-off: Longest prompt, potential over-complexity\n",
    "\n",
    "OBSERVATIONS:\n",
    "- JSON validity should be highest for V2/V3 (more structured)\n",
    "- Accuracy likely improves V1 → V2 → V3\n",
    "- V3 may have best off-by-one accuracy (more nuanced)\n",
    "- API latency increases with prompt length\n",
    "\n",
    "TRADE-OFFS:\n",
    "- Accuracy vs Speed: V3 most accurate but slowest\n",
    "- Consistency vs Simplicity: V1 simplest but less reliable\n",
    "- Token Usage vs Performance: V2/V3 use more tokens but perform better\n",
    "\"\"\")\n",
    "\n",
    "# Save results\n",
    "# sample_df.to_csv('rating_predictions_results.csv', index=False)\n",
    "print(\"\\n✓ Notebook execution complete!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Replace 'YOUR_API_KEY_HERE' with actual Gemini API key\")\n",
    "print(\"2. Update 'yelp.csv' path to your dataset location\")\n",
    "print(\"3. Run initial 10-sample test to verify setup\")\n",
    "print(\"4. Uncomment full evaluation code to process all 200 samples\")\n",
    "print(\"5. Analyze results and document in report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862f5608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1a3b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b12cb23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
